""" Script to parse the eval logs generated by Inspect and generate a CSV file """

import json
import pandas as pd

from tqdm import tqdm
from pathlib import Path
from inspect_ai.log import read_eval_log, EvalLog, EvalScore

curr_dir = Path(__file__).parent.resolve()

def get_score_and_metric(log: EvalLog) -> tuple[EvalScore, float]:
    """ Get the appropriate score and metric value for different types of tasks """
    # NOTE: This is needed because HumanEval has 4 scorers and different metrics from MMLU and TruthfulQA
    if "humaneval" in log.eval.task:
        score = log.results.scores[0]
        assert score.reducer == "mean"
        value = score.metrics["mean"].value
        return score, value
    elif "mmlu" in log.eval.task:
        assert len(log.results.scores) == 1, f"Expected 1 score for task {log.eval.task}, got {len(log.results.scores)}"
        score = log.results.scores[0]
        value = score.metrics["accuracy"].value
        return score, value
    elif "truthfulqa" in log.eval.task:
        assert len(log.results.scores) == 1, f"Expected 1 score for task {log.eval.task}, got {len(log.results.scores)}"
        score = log.results.scores[0]
        value = score.metrics["accuracy"].value
        return score, value
    else:
        raise ValueError(f"Unknown task: {log.eval.task}")

def load_results(results_dir: Path) -> pd.DataFrame:
    result_files = list(results_dir.rglob("*.eval"))
    rows = []

    for result_file in tqdm(result_files):

        # Load the metadata
        with open(result_file.parent / "metadata.json", "r") as f:
            metadata = json.load(f)

        log = read_eval_log(str(result_file))        
        if not log.status == "success":
            print(f"Skipping {result_file.name} because it failed")
            continue

        # Get the task and model
        # NOTE: log.eval.task is of the form inspect_evals/<task_name>
        task_name = log.eval.task.split("/")[-1]
        model_id = log.eval.model

        # Get the metrics
        score, value = get_score_and_metric(log)
        del log

        row = {
            "task_name": task_name,
            "model_id": model_id,
            "suffix": metadata["suffix"],
            "score": value
        }
        for metric_name, metric in score.metrics.items():
            row[metric_name] = metric.value
        rows.append(row)

    df = pd.DataFrame(rows)

    return df

if __name__ == "__main__":

    results_name = "logs"

    # cache the df in a file
    if (curr_dir / f"{results_name}.csv").exists():
        df = pd.read_csv(curr_dir / f"{results_name}.csv")
    else:
        results_dir = curr_dir / results_name
        df = load_results(results_dir)
        df.to_csv(curr_dir / f"{results_name}.csv", index=False)

    # Print number of rows in the dataframe
    print(f"Number of rows: {len(df)}")

    # Print the first 5 rows of the dataframe
    print(df.head())